---
title: "Informe_Regresion_Lineal_Simple_Multiple"
author: "Juan_Narvaez_Siguenza_Daniela"
format: html
editor: visual
---

## Capitulo 2: Aprendizaje estadístico

En esencia, el aprendizaje estadístico se refiere a un conjunto de enfoques para estimar f (una función) y puede implicar más de una variable de entrada.

Hay dos razones principales por las que podemos querer estimar f: predicción e inferencia.

### **1. ¿Cómo estimamos f?**

Siempre supondremos que hemos observado un conjunto de n puntos de datos diferentes. Estas observaciones se denominan datos de entrenamiento porque las utilizaremos para entrenar, o enseñar, a nuestro método a estimar f. Nuestros datos de entrenamiento consisten en {(x1,y1),(x2,y2),\...,(xn,yn)}dondexi =(xi1,xi2,\...,xip)T.

Nuestro objetivo es aplicar un método de aprendizaje estadístico a los datos de entrenamiento para estimar la función f desconocida.

En términos generales, la mayoría de los métodos de aprendizaje estadístico para esta tarea pueden caracterizarse como paramétricos o no paramétricos.

#### **Métodos paramétricos:**

Los métodos paramétricos implican un planteamiento basado en modelos de dos pasos.

1\. En primer lugar, hacemos una suposición sobre la forma funcional de f.

2\. Una vez seleccionado el modelo, necesitamos un procedimiento que utilice los datos de entrenamiento para ajustar o entrenar el modelo.

#### **Métodos no paramétricos:**

Los métodos no paramétricos no hacen suposiciones explícitas sobre la forma funcional de f. En su lugar, buscan una estimación de f que se acerque lo más posible a los puntos de datos sin ser demasiado aproximada o irregular.

### **2. Aprendizaje supervisado frente a aprendizaje no supervisado**

La mayoría de los problemas de aprendizaje estadístico pertenecen a dos categorías: supervisados o no supervisados. Aunque se trata de un tema interesante, queda fuera del alcance de este libro debido a que muchos problemas caen naturalmente dentro de los paradigmas de aprendizaje supervisado o no supervisado.

### **3. Problemas de regresión frente a problemas de clasificación**

Las variables pueden caracterizarse como cuantitativas o cualitativas. Las variables cuantitativas toman valores numéricos. En cambio, las variables cualitativas toman valores en una de K clases o categorías diferentes.  Solemos referirnos a los problemas con una respuesta cuantitativa como problemas de regresión, mientras que los que implican una respuesta cualitativa suelen denominarse problemas de clasificación. Sin embargo, la distinción no siempre es tan clara.

### **4. Evaluación de la precisión de los modelos**

Seleccionar el mejor enfoque puede ser uno de los mayores retos a la hora de llevar a la práctica el aprendizaje estadístico.

### **5. Medición de la calidad del ajuste**

Para evaluar el rendimiento de un método de aprendizaje estadístico en un conjunto de datos determinado, necesitamos alguna forma de medir hasta qué punto sus predicciones se ajustan realmente a los datos observados. Es decir, necesitamos cuantificar hasta qué punto el valor de respuesta predicho para una observación dada se aproxima al valor de respuesta verdadero para esa observación.

### **6. El equilibrio entre sesgo y varianza**

La varianza se refiere a la cantidad en que fˆ cambiaría si la estimáramos utilizando un conjunto de datos de entrenamiento diferente. Sin embargo, si un método tiene una varianza elevada entonces pequeños cambios en los datos de entrenamiento pueden dar lugar a grandes cambios en f. En general, los métodos estadísticos más flexibles tienen mayor varianza.

### **7. El entorno de clasificación**

-   El clasificador de Bayes

Es posible demostrar que la tasa de error de prueba dada en se minimiza, en promedio, por un clasificador muy simple que asigna cada observación a la clase más probable, dados sus valores predictores. El clasificador de Bayes produce la tasa de error de prueba más baja posible, denominada tasa de error de Bayes.

-   K-Nearest Neighbors

Dado un in- teger positivo K y una observación de prueba x0, el clasificador KNN identifica en primer lugar los K puntos de los datos de entrenamiento más cercanos a x0, representados por N0.

Tanto en la regresión como en la clasificación, elegir el nivel correcto de flexibilidad es fundamental para el éxito de cualquier método de aprendizaje estadístico. El equilibrio entre sesgo y varianza, y la forma de U resultante en el error de prueba, pueden hacer que esta tarea sea difícil.

## Capitulo 3: Aprendizaje estadístico

En concreto, la regresión lineal es una herramienta útil para predecir una respuesta cuantitativa.

### **1. Regresión lineal simple**

La regresión lineal simple hace honor a su nombre: es un enfoque muy sencillo para predecir una respuesta cuantitativa Y a partir de una única variable de predicción X.

-   Estimación de los coeficientes

-   Evaluación de la precisión de las estimaciones de los coeficientes

-   Evaluación de la precisión del modelo

Residual Standard Error

R2 Statistic

### **2. Regresión lineal múltiple**

Una opción es realizar tres regresiones lineales simples separadas, cada una de las cuales utilice un medio publicitario diferente como predictor. Sin embargo, el enfoque de ajustar un modelo de regresión lineal simple separado para cada predictor no es del todo satisfactorio.

En lugar de ajustar un modelo de regresión lineal simple separado para cada predictor, un enfoque mejor es ampliar el modelo de regresión lineal simple para que pueda acomodar directamente múltiples predictores. Podemos hacerlo dando a cada predictor un coeficiente de pendiente separado en un único modelo.

### **3. Estimación de los coeficientes de regresión**

Algunas cuestiones importantes

-   Primera: ¿Existe una relación entre la respuesta y los predictores?

-   Dos: Decidir las variables importantes

-   Tres: Ajuste del modelo

-   Cuatro: Predicciones

#### **Predictores cualitativos**

Hay varios predictores cuantitativos: edad, tarjetas, educación, ingresos, límite y calificación.

-   Predictores con sólo dos niveles

-   Predictores cualitativos con más de dos niveles

#### **Extensiones del modelo lineal**

El modelo de regresión lineal estándar (3.19) proporciona resultados interpretables y funciona bastante bien en muchos problemas del mundo real. Sin embargo, hace varias suposiciones muy restrictivas que a menudo se incumplen en la práctica. Dos de los supuestos más importantes establecen que la relación entre los predictores y la respuesta son aditivos y lineales. El supuesto de aditividad significa que la asociación entre un predictor Xj y la respuesta Y no depende de los valores de los demás predictores.

-   Eliminación del supuesto aditivo

-   Relaciones no lineales

#### **Problemas potenciales**

Cuando ajustamos un modelo de regresión lineal a un conjunto de datos concreto, pueden surgir muchos problemas. Los más comunes son los siguientes

1\. No linealidad de las relaciones respuesta-predictor.

2\. Correlación de los términos de error.

3\. Varianza no constante de los términos de error.

4\. Valores atípicos.

5\. Puntos de alto apalancamiento.

6\. Colinealidad.

### **4. Comparación de la regresión lineal con K-Nearest Neighbors**

La regresión lineal es un ejemplo de enfoque paramétrico porque asume una forma funcional lineal para f(X). Los métodos paramétricos tienen varias ventajas. Suelen ser fáciles de ajustar, porque sólo es necesario estimar un pequeño número de coeficientes. En el caso de la regresión lineal, los coeficientes tienen interpretaciones sencillas y las pruebas de significación estadística pueden realizarse fácilmente. Pero los métodos paramétricos tienen una desventaja: por construcción, hacen fuertes suposiciones sobre la forma de f(X). Por el contrario, los métodos no paramétricos no asumen explícitamente una forma paramétrica para f(X) y, por lo tanto, proporcionan un enfoque alternativo y más flexible para realizar la regresión.

Por regla general, los métodos paramétricos tienden a superar a los no paramétricos cuando el número de observaciones por predictor es pequeño.

Incluso cuando la dimensión es pequeña, podríamos preferir la regresión lineal a KNN desde el punto de vista de la interpretabilidad. Si el MSE de prueba de KNN es sólo ligeramente inferior al de la regresión lineal, podríamos estar dispuestos a renunciar a un poco de precisión en la predicción por el bien de un modelo simple que puede ser descrito en términos de unos pocos coeficientes, y para el que los valores p están disponibles.

## Lab: Regresión lineal

### Librerías

La función library( ), se utiliza para cargar bibliotecas, o grupos de funciones.

```{r}
library(MASS)
library(ISLR2)
```

Regresión lineal simple

```{r}
head(Boston)
```

La función lm( ), sirve para ajustar un modelo de regresión lineal simple


```{r}
lm.fit <- lm(medv ~ lstat , data = Boston)
attach (Boston)
lm.fit <- lm(medv ~ lstat)
```

### Regresión lineal

Si usamos lm.fit, obtendremos información básica del modelo; sin embargo, el summary( ), para información más detallada

```{r}
lm.fit
```

```{r}
summary(lm.fit)
```

La función names( ), sirve para encontrar otras piezas de información en el almacenamiento de lm.fit

```{r}
names(lm.fit)
```

Podemos ocupar el extractor de funciones "coef( )" para acceder a ellos.

```{r}
coef(lm.fit)
```

La función confint( ), sirve para obtener intervalos de confianza para coeficientes estimados

```{r}
confint(lm.fit)
```

La función predict( ), sirve para producir intervalos de confianza e intervalos de predicción.

```{r}
predict(lm.fit, data.frame(lstat= (c(5, 10, 15))), interval = "confidence")
```

```{r}
predict(lm.fit, data.frame(lstat= (c(5, 10, 15))), interval = "prediction")
```

Para obtener un intervalo de confianza para las estimaciones de los coeficientes, podemos usar abline( ).

La función abline( ), dibuja una línea.

```{r}
plot(lstat, medv)
abline(lm.fit)
```

la función abline(a, b), sirve para dibujar una línea con intercepción a y pendiente b. El comando lwd hace que el ancho de la regresión lineal aunmente según el número que se determine.

"pch" crea diferentes símbolos de trazado.

```{r}
plot(lstat, medv)
abline (lm.fit, lwd = 3)
abline (lm.fit, lwd = 3, col = "red" )
plot (lstat , medv , col = " red ")
plot (lstat , medv , pch = 20)
plot (lstat , medv , pch = "+")
plot (1:20, 1:20, pch = 1:20)
```

Las funciones par( ), mfrow( ) sirven para mostrar cuatro gráficos juntos.

```{r}
par (mfrow = c(2, 2))
plot (lm.fit)
```

La función residuals( ), sirve para calcular los residuos de un ajuste de regresión lineal, mientras que la función rstudent( ), devolverá los residuos estudentizados.

```{r}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

La función hatvalues( ), puede calcular cualquier número de predictores. La función "which.max" identifica el índice del elemento más grande de un vector.

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

### Regresión lineal múltiple

La función lm( ), sirve para ajustar un modelo de regresión lineal múltiple usando mínimos cuadrados.

```{r}
lm.fit <- lm(medv ~ lstat + age , data = Boston)
summary (lm.fit)
```

En lugar de escribir todas las variables podemos utilizar la siguiente abreviatura.

```{r}
lm.fit <- lm(medv ~ ., data = Boston)
summary (lm.fit)
```

La función vif( ), puede utilizarse para calcular los factores de inflación de la varianza, además debe instalarse el package "car".

```{r}
library (car)
vif (lm.fit)
```

#### Regresión lineal

Ejecutar una regresión lineal excluyendo un predictor, en este caso sería la edad.

```{r}
lm.fit1 <- lm(medv ~ . - age , data = Boston)
summary (lm.fit1)
```

Tambien se puede utilizar la función update( ).

```{r}
 lm.fit1 <- update (lm.fit , ~ . - age)
```

### Términos de interacción

```{r}
summary (lm(medv ~ lstat * age , data = Boston))
```

### Transformaciones no lineales de los predictores

Lm( ) puede acomodar transformaciones no lineales de los predictores, la función I( ), el uso estándar en R (poner un exponencial de 2).

```{r}
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary (lm.fit2)

```

La función anova( ), realiza una prueba de hipótesis comparando los dos modelos.

```{r}
lm.fit <- lm(medv ~ lstat)
anova (lm.fit , lm.fit2)
```

```{r}
par (mfrow = c(2, 2))
plot (lm.fit2)
```

Para crear un ajuste cúbico, podemos incluir un predictor de la forma I(x\^3), para un mejor enfoque podemos utilizar la función poly( ), que crea un polinomio dentro del lm( ).

```{r}
lm.fit5 <- lm(medv ~ poly (lstat , 5))
summary (lm.fit5)
```

Realizamos la transformación de un registro.

```{r}
summary (lm(medv ~ log(rm), data = Boston))
```

### Predictores cualitativos

Los Caarseats( ) forman parte del ISRL2, los datos incluyen predictores cualitativos como "Shelveloc", un indicador de la calidad de la ubicación de las estanterías.

```{r}
head (Carseats)
```

```{r}
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age ,
data = Carseats)
summary (lm.fit)
```

La función contrasts( ), devuelve la codificación que R usa para las variables ficticias.

```{r}
attach (Carseats)
contrasts (ShelveLoc)
```

### Funciones de escritura


Cargamos las librerías, con el Enter podemos ingresar muchos comandos y finalmente R informará que no se puede introducir más comandos.

```{r}
LoadLibraries <- function () {
+ library (ISLR2)
+ library (MASS)
+ print ("The libraries have been loaded .")}
```

Ahora si escribimos la funci\[on LoadLibraries, R nos dirá que hay en la función.

```{r}
LoadLibraries
  function() {
  library(ISLR2)
  library(MASS)
  print("The libraries have been loaded.")
}
```

Si llamamos a la función, las bibliotecas se cargan y se genera la declaración de impresión.

```{r}
#LoadLibraries()
```